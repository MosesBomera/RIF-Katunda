{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Detection: An End to End Training Pipeline (Part Three).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou4hgIJLDEbk",
        "colab_type": "text"
      },
      "source": [
        "# Object Detection: An End to End Training Pipeline (Part Three)\n",
        "In this notebook, we shall finally train an object detection model, using the [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). \n",
        "***\n",
        "**Objectives:**\n",
        "1. Select and configure a pretrained model.\n",
        "2. Initiate `Tensorboard` to monitor the training process.\n",
        "3. Train an object detection model.\n",
        "4. Export the resulting model.\n",
        "**NOTE: The notebook has a number of TO-DO exercises for you to complete, please endeavor to attempt them.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ett17wL5z1c7",
        "colab_type": "text"
      },
      "source": [
        "We shall borrow a number of steps from the previous notebooks especially `Object Detection: Environment Setup and Creating TFRecords (Part Two)`, including:\n",
        "- Environment setup\n",
        "- Loading the dataset.\n",
        "- Setting the workspace.\n",
        "- Converting the dataset to TFRecords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiUPS96C3OsL",
        "colab_type": "text"
      },
      "source": [
        "Steps:\n",
        "1. We shall configure the notebook as we did in the last notebook upto conversion to TFecords.\n",
        "2. Download and configure a pretrained model.\n",
        "3. Initiate `Tensorboard` for training monitoring.\n",
        "4. Train the model.\n",
        "5. Export the resulting model for object detection tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqU1Usfl5aFJ",
        "colab_type": "text"
      },
      "source": [
        "### 0.0 Notebook Configuration\n",
        "As in the past notebooks, in this section, we install all the necessary libraries that are needed for use to train an object detection model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbK2uk5HB3V1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title [IMPORTANT] Run Cell to Set up Environment {display-mode: \"form\"}\n",
        "%%capture\n",
        "# 1. Create a directory named `object_detection`, this is where the model files will be stored.\n",
        "# 2. Download and install the Tensorflow Object Detection API models using [Git](https://en.wikipedia.org/wiki/Git) into the `object_detection` folder.\n",
        "# 3. Install the required libraries for the API.\n",
        "# 4. Install TF-Slim, a lightweight library for defining, training and evaluating complex models in TensorFlow.\n",
        "\n",
        "# We shall import the os library, it provides a number of functions that allow \n",
        "# interaction with the operating environment.\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "import glob\n",
        "import urllib\n",
        "import io\n",
        "import xml.etree.ElementTree as ET\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "import csv\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import tensorflow.compat.v1 as tf\n",
        "from collections import namedtuple\n",
        "\n",
        "# 1. Create a directory named detection\n",
        "!mkdir detection\n",
        "os.chdir('detection')\n",
        "\n",
        "# 2a. Download and install the Tensorflow Object Detection API models using Git into the object_detection folder,\n",
        "# we use git command to get the source files from GitHub, git is preinstalled in the Google Colab environment.\n",
        "!git clone --depth 1 https://github.com/tensorflow/models.git\n",
        "\n",
        "# 2b. Add the folder to the path, this allows to import \n",
        "#     scripts as we do with install libraries\n",
        "os.environ['PYTHONPATH'] += ':'+'/content/detection/models'\n",
        "\n",
        "# 3. Install the required libraries for the API. The libraries are installed\n",
        "#    from the setup.py.\n",
        "\n",
        "# 3a. First we shall move into research/models, this is where the setup.py script\n",
        "#     is stored.\n",
        "%cd models/research\n",
        "\n",
        "# Import dataset preparations from the object detection folder\n",
        "from object_detection.utils import dataset_util\n",
        "\n",
        "# 3b. This commands the environment to run the setup.py script if it exists.\n",
        "!pip install .\n",
        "\n",
        "# 4. Install TF-Slim, a lightweight library for defining, training and evaluating complex models in TensorFlow.\n",
        "!pip install tf_slim\n",
        "\n",
        "# 5a. Install the protobuf dependencies.\n",
        "!protoc object_detection/protos/*.proto --python_out=.;\n",
        "pwd = os.getcwd();\n",
        "\n",
        "# 5b. Add TF_slim to the system path.\n",
        "os.environ['PYTHONPATH'] += f':{pwd}:{pwd}/slim';\n",
        "\n",
        "# 6. Move back to the object_detection folder.\n",
        "%cd ../../\n",
        "\n",
        "def split_indices(x, train=0.8, test=0.0, validate=0.2, shuffle=True):\n",
        "    \"\"\"\n",
        "      Returns the indices at which the data is split.\n",
        "    \"\"\"\n",
        "    # split training data\n",
        "    n = len(x)\n",
        "    v = np.arange(n)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(v)\n",
        "\n",
        "    i = round(n * train)  # train\n",
        "    j = round(n * test) + i  # test\n",
        "    k = round(n * validate) + j  # validate\n",
        "    return v[:i], v[i:j], v[j:k]  # return indices\n",
        "\n",
        "def split_files(file_names,train=0.8, test=0.2, validate=0.0):\n",
        "    \"\"\"\n",
        "      Split the files provided according to the specified distributions.\n",
        "\n",
        "      file_names  this is a list of file names that are split.\n",
        "      train       the distribution for the train files, 0 <= x <= 1\n",
        "      test        the distribution for the test files, it should complement train to add to one.\n",
        "                  i.e. if train is 0.8, test should be 0.2 such that 0.8 + 0.2 = 1.0\n",
        "      validate    this is the distribution for the validation set, it should also \n",
        "                  complement the train and test values i.e if train is 0.8, test is 0.1, validate\n",
        "                  should be 0.1 such that 0.8 + 0.1 + 0.1 = 1.0\n",
        "\n",
        "      Returns:\n",
        "        (tuple) train, test, val\n",
        "                train   a list containing the file names of the train set\n",
        "                test    a list containing the file names of the test set\n",
        "                val     a list containing the file name of the validation set\n",
        "    \"\"\"\n",
        "    # split training data\n",
        "    file_name = list(filter(lambda x: len(x) > 0, file_names))\n",
        "    file_name = sorted(file_name)\n",
        "    i, j, k = split_indices(file_names, train=train, test=test, validate=validate)\n",
        "    train = []\n",
        "    test = []\n",
        "    val = []\n",
        "    datasets = {'train': i, 'test': j, 'val': k}\n",
        "    for key, item in datasets.items():\n",
        "        if item.any():\n",
        "            for ix in item:\n",
        "                if key == 'train':\n",
        "                    train.append(file_names[ix])\n",
        "                if key == 'test':\n",
        "                    test.append(file_names[ix])\n",
        "                if key == 'val':\n",
        "                    val.append(file_names[ix])\n",
        "\n",
        "    return train, test, val\n",
        "\n",
        "def json_to_csv(file_names, images_dir, labels_path, annotations_dir, label_file_name):\n",
        "  \"\"\"\n",
        "  Converts a JSON file to a csv file.\n",
        "\n",
        "  file_names       list of file names.\n",
        "  images_dir       path to the images directory\n",
        "  labels_path      is the path to the labels JSON file.\n",
        "  annotations_dir  is the directory in which the annotations will be stored.\n",
        "  label_file_name  is the name of the output .csv file i.e. labels_train.csv\n",
        "  \"\"\"\n",
        "  # Load coco file\n",
        "  f = open(labels_path, 'r')\n",
        "  COCO_DATA = json.load(f)\n",
        "  f.close()\n",
        "\n",
        "  images = COCO_DATA[\"images\"]\n",
        "  annotations = COCO_DATA[\"annotations\"]\n",
        "      \n",
        "  # Generating the csv in the annotations folder under data directory. (Ideally)\n",
        "  csv_file_name = os.path.join(str(annotations_dir), label_file_name)\n",
        "\n",
        "  class_name = 'brownspot' # Normally, there will be more than one class, extract accordingly.\n",
        "\n",
        "  with open(csv_file_name, 'w') as csv_label_file:\n",
        "    f = csv.writer(csv_label_file)\n",
        "    f.writerow(['file_name', 'width', 'height', 'class', 'xmin', 'ymin', \n",
        "                'xmax', 'ymax'])\n",
        "\n",
        "    for file_name in tqdm(file_names, desc = \"Processing CSV\"):\n",
        "        id = None\n",
        "        for image in images:\n",
        "          if file_name == image['file_name']:\n",
        "            id = image['id']\n",
        "\n",
        "        im = Image.open(os.path.join(images_dir, file_name))\n",
        "        width, height = im.size\n",
        "\n",
        "        for annotation in annotations:\n",
        "            if id == annotation['image_id']:\n",
        "                bbox = annotation['bbox']\n",
        "\n",
        "                # COCO bbox label format: [xmin, ymin, width, height]\n",
        "                xmin = bbox[0]\n",
        "                xmax = bbox[0] + bbox[2]\n",
        "                ymin = bbox[1]\n",
        "                ymax = bbox[1] + bbox[3]\n",
        "\n",
        "                # Write to .csv file.\n",
        "                f.writerow([file_name, width, height, class_name, xmin, ymin, xmax, ymax])  \n",
        "\n",
        "def create_pbtxt(annotations_dir):\n",
        "  \"\"\"\n",
        "    Creates a pbtxt file.\n",
        "\n",
        "    TensorFlow requires a label map, which maps each of the used labels \n",
        "    to an integer values. This label map is used both by the training and detection \n",
        "    processes. Notice the labels are one-indexed i.e. start at 1 (one).\n",
        "\n",
        "    Example:\n",
        "    # example.pbtxt\n",
        "    item {\n",
        "      id: 1\n",
        "      name: 'cat'\n",
        "    }\n",
        "\n",
        "    item {\n",
        "      id: 2\n",
        "      name: 'dog'\n",
        "    }\n",
        "\n",
        "    The file is stored under the annotations folder.\n",
        "  \"\"\"\n",
        "  # Create the label map\n",
        "  label_map_path = os.path.join(annotations_dir, \"label_map.pbtxt\")\n",
        "  pbtxt_content = \"\"\n",
        "\n",
        "  class_name = 'brownspot' # Could be more than one class name.\n",
        "\n",
        "  pbtxt_content = (\n",
        "      pbtxt_content\n",
        "      + \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\\n\".format(1, class_name)\n",
        "  )\n",
        "  pbtxt_content = pbtxt_content.strip()\n",
        "  with open(label_map_path, \"w\") as f:\n",
        "      f.write(pbtxt_content)\n",
        "\n",
        "def move_files(files, source, dest):\n",
        "  \"\"\"Move files from the source directory to the destination directory.\"\"\"\n",
        "  for filename in files:\n",
        "    copyfile(os.path.join(source, filename),\n",
        "                 os.path.join(dest, filename))\n",
        "    \n",
        "def class_text_to_int(row_label):\n",
        "  if row_label == 'brownspot': # the respective class_name\n",
        "    return 1\n",
        "  else:\n",
        "    None\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    # check if the image format is matching with your images.\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "def create_tfrecord(labels_file, images_dir, annotations_dir):\n",
        "  \"\"\"\n",
        "    Create a tfrecord from the .csv file.\n",
        "  \"\"\"\n",
        "  csv_name = labels_file.split('/')[-1][:-4]\n",
        "  writer = tf.io.TFRecordWriter(os.path.join(annotations_dir, csv_name + '.record'))\n",
        "  path = os.path.join(images_dir)\n",
        "  examples = pd.read_csv(labels_file)\n",
        "  grouped = split(examples, 'file_name')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "  writer.close()\n",
        "  print('Successfully created the TFRecords: {}'.format(csv_name + '.record'))\n",
        "\n",
        "# The training helper function.\n",
        "def train_model(path):\n",
        "  \"\"\"\n",
        "    This is a helper function to aid with the training.\n",
        "  \"\"\"\n",
        "  !python workspace/data/model_main_tf2.py \\\n",
        "    --model_dir=workspace/data/training \\\n",
        "    --pipeline_config_path={path}\n",
        "\n",
        "# The model exporter helper function.\n",
        "def export_model(config_path, checkpoint_dir, export_dir):\n",
        "  \"\"\"\n",
        "    This function calls the model exporter script that creates the graph format\n",
        "    of the model that can be exported.\n",
        "  \"\"\"\n",
        "  !python workspace/data/exporter_main_v2.py \\\n",
        "    --input_type image_tensor \\\n",
        "    --pipeline_config_path {config_path} \\\n",
        "    --trained_checkpoint_dir {checkpoint_dir} \\\n",
        "    --output_directory {export_dir}\n",
        "\n",
        "\n",
        "# Finally, we prepare the workspace folders as discussed in the last notebook. \n",
        "# Refer to it for details.\n",
        "# Let's go ahead and create the folders.\n",
        "!mkdir workspace\n",
        "!mkdir workspace/data\n",
        "!mkdir workspace/data/annotations\n",
        "!mkdir workspace/data/images\n",
        "!mkdir workspace/data/images/train\n",
        "!mkdir workspace/data/images/test \n",
        "!mkdir workspace/data/pre-trained-model\n",
        "!mkdir workspace/data/training\n",
        "\n",
        "# Copy the model_main.py into the object_detection/workspace/data\n",
        "# This is to aid with the training later on.\n",
        "copyfile('/content/detection/models/research/object_detection/model_main_tf2.py',\\\n",
        "         '/content/detection/workspace/data/model_main_tf2.py')\n",
        "\n",
        "copyfile('/content/detection/models/research/object_detection/exporter_main_v2.py', \\\n",
        "         '/content/detection/workspace/data/exporter_main_v2.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVIr873T7HR8",
        "colab_type": "text"
      },
      "source": [
        "Next, we shall upload the dataset shared for this notebook, the dataset contains 100 images of passion fruit leaves diseased with the [Brown Spot Disease](https://rifkatunda.github.io/knowledge-center/#brownspot) and the corresponding labels shared in the `COCO` format.\n",
        "***\n",
        "In this notebook, we shall train a model to detect the brownspot patches on each leaf. (Refer to the first notebook `Object Detection: Data Parsing and Visualization (Intro: One)` for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wIDNMI97GlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. The dataset is uploaded to the content directory, to unzip it, we have two \n",
        "#    options, either to move back in the directory tree or use the full path.\n",
        "# We shall use the full path in this notebook.\n",
        "!unzip /content/dataset.zip\n",
        "\n",
        "# This function, unzips the folder to your current directory, so it is important\n",
        "# to always know, where you are in the directory tree.\n",
        "# Use the !pwd command to find out."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMAhbu8r9wVY",
        "colab_type": "text"
      },
      "source": [
        "In the next step, we shall follow a couple sub-steps as listed below:\n",
        "- Split the dataset into the train set and the test set. \n",
        "- Create the respective labels.csv for the test and train set.\n",
        "- Create the `label_map.pbtxt` file.\n",
        "- Move files to the respective prepared directories.\n",
        "- Generate TFRecords from the datasets.\n",
        "***\n",
        "**NOTE: This sub-section is left as a TO-DO exercise for you. Please refer to `Object Detection: Environment Setup and Creating TFRecords (Part Two)` for a recap.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdLM65l9vyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0. Get the list of the files in the images directory.\n",
        "files = os.listdir('/content/detection/dataset/images')\n",
        "\n",
        "# [TO-DO]\n",
        "# 1. Split the dataset into the train set and test set.\n",
        "\n",
        "# 2. Create the respective labels.csv for the test and train set.\n",
        "\n",
        "# 3. Create the label_map.pbtxt file.\n",
        "\n",
        "# 4. Move files to the respective prepared directories.\n",
        "\n",
        "# 5. Generate TFRecords for the train set.\n",
        "\n",
        "# 6. Generate TFRecords for the test set.\n",
        "\n",
        "# NOTE: Please refer to the last notebook [Object Detection: Environment Setup \n",
        "# and Creating TFRecords (Part Two)] for a recap."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM19QcVFDWVc",
        "colab_type": "text"
      },
      "source": [
        "### 1.0 Select and configure a pretrained model.\n",
        "We shall use pretrained weights (randomly choosen from the [TensorFlow’s detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), this is an online storage platform of pre-trained `Tensorflow` models). For this tutorial, we shall use [ SSD ResNet50 V1 FPN 640x640 ](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz), pretrained weights. This is an application of transfer learning which is used in training scenarios when there isn't enough training data, like in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os91OSqDEnuS",
        "colab_type": "text"
      },
      "source": [
        "Model configuration steps:\n",
        "Steps to follow:\n",
        "1. Download the pretrained weights from [TensorFlow’s detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
        "2. Edit the config file to fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TNmGQVoFAZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Download the pretrained weights of your choice from TensorFlow’s detection model zoo.\n",
        "PRE_TRAINED_MODEL_DIR = r'/content/detection/workspace/data/pre-trained-model'\n",
        "MODEL = 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
        "\n",
        "!wget -P {PRE_TRAINED_MODEL_DIR} http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -zxvf {PRE_TRAINED_MODEL_DIR}/{MODEL} -C {PRE_TRAINED_MODEL_DIR} # Extract to a given directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBoaw83VFX8h",
        "colab_type": "text"
      },
      "source": [
        "Finally, open and edit the config file to fit the dataset. The config file also contains numerous hyperparameters that will be continously optimized for the model. But for this tutorial, we shall focus on the following parameters.\n",
        "- `num_classes`: This specifies the number of classes in your dataset, for this dataset we have only one class.\n",
        "- `batch_size`: specifies the number of images loaded for one iteration, try a smaller batch size if the training hangs.\n",
        "- `fine_tune_checkpoint`: this is a path to the directory with the pretrained model, that we downloaded in the last step.\n",
        "- `input_path`: this is under the `train_input_reader` or `eval_input_reader` section, and specifies the path to the training data or testing data respectively.\n",
        "- `label_map_path`: this is also under the `train_input_reader` and or `eval_input_reader`, and specifies the path to the `label_map.pbtxt` file that we created earlier in both sections.\n",
        "- `data_augmentation_options`: This is an interesting section to tweak and left as an exercise to the try out a number of augmentation strategies. Data Augmentation is a technique used to artificially increase data used for training, apart from increasing the data points, the model generalizes better on unseen data.\n",
        "- `num_steps`: This is the flag used to set the number of epochs, say you have have 50 images, and a batch of 2 images per step, then it takes 25 steps to go through all 50 images once. This is then one epoch. So if you want to train for 100 epochs you just need to set the number of steps to 25 * 100 = 2500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHV7RAliFMd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile /content/detection/workspace/data/pre-trained-model/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config\n",
        "\n",
        "# Open the file to see it's contents.\n",
        "# Faster R-CNN with Inception v2, configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1 # Change to the number of classes in your dataset.\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        # Change to the your image set resolution, for our dataset, it is 400x400\n",
        "        height: 400  \n",
        "        width: 400\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: \"ssd_resnet50_v1_fpn_keras\"\n",
        "      depth_multiplier: 1.0\n",
        "      pad_to_multiple: 32\n",
        "      min_depth: 16\n",
        "      conv_hyperparams {\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00039999998989515007\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            mean: 0.0\n",
        "            stddev: 0.029999999329447746\n",
        "          }\n",
        "        }\n",
        "        activation: RELU_6\n",
        "        batch_norm {\n",
        "          decay: 0.996999979019165\n",
        "          scale: true\n",
        "          epsilon: 0.0010000000474974513\n",
        "        }\n",
        "      }\n",
        "      override_base_feature_extractor_hyperparams: true\n",
        "      fpn {\n",
        "        min_level: 3\n",
        "        max_level: 7\n",
        "      }\n",
        "    }\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "        use_matmul_gather: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      weight_shared_convolutional_box_predictor {\n",
        "        conv_hyperparams {\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00039999998989515007\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            random_normal_initializer {\n",
        "              mean: 0.0\n",
        "              stddev: 0.009999999776482582\n",
        "            }\n",
        "          }\n",
        "          activation: RELU_6\n",
        "          batch_norm {\n",
        "            decay: 0.996999979019165\n",
        "            scale: true\n",
        "            epsilon: 0.0010000000474974513\n",
        "          }\n",
        "        }\n",
        "        depth: 256\n",
        "        num_layers_before_predictor: 4\n",
        "        kernel_size: 3\n",
        "        class_prediction_bias_init: -4.599999904632568\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      multiscale_anchor_generator {\n",
        "        min_level: 3\n",
        "        max_level: 7\n",
        "        anchor_scale: 4.0\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        scales_per_octave: 2\n",
        "      }\n",
        "    }\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 9.99999993922529e-09\n",
        "        iou_threshold: 0.6000000238418579\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "        use_static_shapes: false\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    loss {\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      classification_loss {\n",
        "        weighted_sigmoid_focal {\n",
        "          gamma: 2.0\n",
        "          alpha: 0.25\n",
        "        }\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    encode_background_as_zeros: true\n",
        "    normalize_loc_loss_by_codesize: true\n",
        "    inplace_batchnorm_update: true\n",
        "    freeze_batchnorm: false\n",
        "  }\n",
        "}\n",
        "train_config {\n",
        "  batch_size: 4 # Increase and decrease depending on memory available.\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    random_crop_image {\n",
        "      min_object_covered: 0.0\n",
        "      min_aspect_ratio: 0.75\n",
        "      max_aspect_ratio: 3.0\n",
        "      min_area: 0.75\n",
        "      max_area: 1.0\n",
        "      overlap_thresh: 0.0\n",
        "    }\n",
        "  }\n",
        "  sync_replicas: true\n",
        "  optimizer {\n",
        "    momentum_optimizer {\n",
        "      learning_rate {\n",
        "        cosine_decay_learning_rate {\n",
        "          learning_rate_base: 0.03999999910593033\n",
        "          total_steps: 25000\n",
        "          warmup_learning_rate: 0.013333000242710114\n",
        "          warmup_steps: 2000\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.8999999761581421\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/detection/workspace/data/pre-trained-model/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
        "  num_steps: 25000\n",
        "  startup_delay_steps: 0.0\n",
        "  replicas_to_aggregate: 8\n",
        "  max_number_of_boxes: 100\n",
        "  unpad_groundtruth_tensors: false\n",
        "  fine_tune_checkpoint_type: \"detection\" # Set this to \"detection\" since we want to be training the full detection model\n",
        "  use_bfloat16: false  # Set this to false if you are not training on a TPU\n",
        "  fine_tune_checkpoint_version: V2\n",
        "}\n",
        "train_input_reader {\n",
        "  label_map_path: \"/content/detection/workspace/data/annotations/label_map.pbtxt\" # Path to the label_map\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/detection/workspace/data/annotations/train_labels.record\" # Path to training TFRecord\n",
        "  }\n",
        "}\n",
        "eval_config {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  use_moving_averages: false\n",
        "}\n",
        "eval_input_reader {\n",
        "  label_map_path: \"/content/detection/workspace/data/annotations/label_map.pbtxt\" # Path to the label_map\n",
        "  shuffle: false\n",
        "  num_epochs: 1\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/detection/workspace/data/annotations/test_labels.record\" # Path to the test set.\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSJvVH7aJYkk",
        "colab_type": "text"
      },
      "source": [
        "### 2.0 Initiate `Tensorboard` for training monitoring.\n",
        "Tensorflow provides the [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) utility library that allows you to continuously monitor and visualize a number of different training/evaluation metrics.\n",
        "***\n",
        "If you run Tensorboard before training the model, then you are able to monitor progress concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmXRoxfdHWoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Run the command below to run tensorboard, --logdir should point to the folder that stores the \n",
        "#    checkpoints during training, in this case it is the training folder under the data directory.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir workspace/data/training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbwpv9XbK58B",
        "colab_type": "text"
      },
      "source": [
        "### 3.0 Training the Model\n",
        "Finally we get to train the model. For the training process, we shall work in the `workspace/data` directory.\n",
        "***\n",
        "Steps:\n",
        "- First, we shall copy the `object_detection/models/research/object_detection/model_main.py` file to `object_detection/workspace/data` folder.\n",
        "- Next, we shall move into the `workspace/data` directory.\n",
        "- Then we shall call the `train_model(config_path)` function, this is the function used to train the model, it expects as argument the path to the config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8D65596KOCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO-DO\n",
        "# 1. Set the path argument with the path to the config file.\n",
        "path = ''\n",
        "\n",
        "# 2. Call the train_model function with the set path."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pchwsMFp9Qmu",
        "colab_type": "text"
      },
      "source": [
        "### 4.0 Exporting a Trained Model\n",
        "With the training job complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection.\n",
        "***\n",
        "For this, we shall call the `export_model(config_path, checkpoint_dir, export_dir)` function, defined in the notebook configuration section. A few details about the arguments:\n",
        "- `config_path`: This is the path to the configuration file, the same one used in the training step.\n",
        "- `checkpoint_dir`: This is the path to the directory that stores the checkpoints as the model is trained, in this case, it is `/content/detection/workspace/data/training`.\n",
        "- `export_dir`: This specifies the path to directory in which the exported model should be stored. If it doesn't exist, it is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-bjWHsK48Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO-DO\n",
        "# 1. Set the arguments to the corresponding values.\n",
        "path = r''\n",
        "checkpoint_dir = r'\n",
        "export_dir = r''\n",
        "\n",
        "# 2. Call the export_model function and pass it the arguments as defined."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJEkBa_SERHZ",
        "colab_type": "text"
      },
      "source": [
        "After the above process has completed, you should find a new folder `exported_model` under the `workspace/data` directory, that has the following structure:\n",
        "```\n",
        "/data\n",
        "├─ ...\n",
        "├─ exported-model/\n",
        "│  ├─ checkpoint/\n",
        "│  ├─ saved_model/\n",
        "│  └─ pipeline.config\n",
        "└─ ...\n",
        "```\n",
        "The exported model is under the `saved_model` folder, and it is named `saved_model.pb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoYXqTehDIK1",
        "colab_type": "text"
      },
      "source": [
        "That is it for the part three, for more details. Please checkout the official [TensorFlow 2 Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)."
      ]
    }
  ]
}